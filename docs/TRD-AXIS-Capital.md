# Technical Requirements Document (TRD)
## AXIS Capital - AI Futures Trading System

**Version**: 2.0 (Final)
**Date**: 2025-10-27
**Related**: PRD-AXIS-Capital.md
**Status**: Ready for Implementation

---

## 1. System Architecture

### 1.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Load Balancer (Nginx)                   â”‚
â”‚                 SSL/TLS Termination                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI    â”‚  â”‚     n8n      â”‚  â”‚   Celery     â”‚
â”‚  (Backend)   â”‚  â”‚  (Workflows) â”‚  â”‚  (Workers)   â”‚
â”‚  Port: 8000  â”‚  â”‚  Port: 5678  â”‚  â”‚  Queue: Redisâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                 â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PostgreSQL   â”‚  â”‚    Redis     â”‚  â”‚ TimescaleDB  â”‚
â”‚ (Main DB)    â”‚  â”‚   (Cache)    â”‚  â”‚ (Timeseries) â”‚
â”‚ Port: 5432   â”‚  â”‚  Port: 6379  â”‚  â”‚ Port: 5433   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Component Responsibilities

| Component | Purpose | Technology |
|-----------|---------|------------|
| **FastAPI** | REST API, ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ | Python 3.11, FastAPI |
| **n8n** | AI Agent workflows | n8n (Docker) |
| **Celery** | ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…, ìŠ¤ì¼€ì¤„ë§ | Celery + Redis |
| **PostgreSQL** | ê±°ë˜ ê¸°ë¡, ì„¤ì • ì €ì¥ | PostgreSQL 15 |
| **Redis** | ìºì‹±, Celery Broker | Redis 7 |
| **TimescaleDB** | OHLCV ì‹œê³„ì—´ ë°ì´í„° | TimescaleDB (PostgreSQL extension) |
| **Grafana** | ëŒ€ì‹œë³´ë“œ, ëª¨ë‹ˆí„°ë§ | Grafana + Prometheus |

---

## 2. Technology Stack

### 2.1 Backend (FastAPI)

```yaml
Language: Python 3.11+
Framework: FastAPI 0.104+

Core Libraries:
  - ccxt: ê±°ë˜ì†Œ API í†µí•©
  - pandas: ë°ì´í„° ì²˜ë¦¬
  - numpy: ìˆ˜ì¹˜ ê³„ì‚°
  - ta-lib: ê¸°ìˆ  ì§€í‘œ
  - pydantic: ë°ì´í„° ê²€ì¦
  - sqlalchemy: ORM
  - redis: ìºì‹±

Structure:
  api/
    â”œâ”€â”€ main.py              # FastAPI ì•±
    â”œâ”€â”€ routers/
    â”‚   â”œâ”€â”€ trading.py       # ê±°ë˜ ì‹¤í–‰
    â”‚   â”œâ”€â”€ portfolio.py     # í¬íŠ¸í´ë¦¬ì˜¤ ì¡°íšŒ
    â”‚   â””â”€â”€ data.py          # ë°ì´í„° API
    â”œâ”€â”€ services/
    â”‚   â”œâ”€â”€ binance.py       # ë°”ì´ë‚¸ìŠ¤ API
    â”‚   â”œâ”€â”€ risk.py          # ë¦¬ìŠ¤í¬ ê³„ì‚°
    â”‚   â””â”€â”€ indicators.py    # ì§€í‘œ ê³„ì‚°
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ position.py      # Position ëª¨ë¸
    â”‚   â””â”€â”€ trade.py         # Trade ëª¨ë¸
    â””â”€â”€ core/
        â”œâ”€â”€ config.py        # ì„¤ì •
        â””â”€â”€ database.py      # DB ì—°ê²°
```

### 2.2 Workflow Engine (n8n)

```yaml
Version: n8n 1.0+

Workflows:
  1. Main-Trading-Workflow.json
     - CEO â†’ Analysts â†’ Risk â†’ Execution

  2. BTC-Analysis-Workflow.json
     - BTC ì „ë¬¸ ë¶„ì„

  3. ETH-Analysis-Workflow.json
     - ETH ì „ë¬¸ ë¶„ì„ (ì¡°ê±´ë¶€)

AI Integration:
  - OpenAI API (GPT-4o, GPT-o1)
  - Custom HTTP Request nodes
  - Python Code nodes
```

### 2.3 Background Tasks (Celery)

```yaml
Version: Celery 5.3+
Broker: Redis
Result Backend: Redis

Task Categories:
  1. Data Collection (ë†’ì€ ë¹ˆë„)
     - collect_btc_ohlcv (5ë¶„)
     - collect_funding_rates (5ë¶„)
     - collect_eth_ohlcv (5ë¶„)

  2. Monitoring (ì¤‘ê°„ ë¹ˆë„)
     - monitor_positions (1ë¶„)
     - check_liquidation_risk (1ë¶„)
     - update_portfolio_value (10ë¶„)

  3. Trading Cycle (ë‚®ì€ ë¹ˆë„)
     - trigger_trading_cycle (CEO ê²°ì •)
     - check_regime_change (1ì‹œê°„)
```

---

## 3. Database Schema

### 3.1 PostgreSQL Schema

#### users
```sql
CREATE TABLE users (
    user_id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    api_key_encrypted TEXT NOT NULL,
    api_secret_encrypted TEXT NOT NULL,
    risk_profile VARCHAR(20) DEFAULT 'balanced',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- risk_profile: conservative, balanced, aggressive
```

#### positions
```sql
CREATE TABLE positions (
    position_id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(user_id),
    exchange VARCHAR(20) DEFAULT 'binance',
    symbol VARCHAR(20) NOT NULL,
    side VARCHAR(10) NOT NULL,  -- LONG or SHORT
    leverage DECIMAL(5,2) NOT NULL,

    -- Entry
    entry_price DECIMAL(20,8) NOT NULL,
    entry_time TIMESTAMP NOT NULL,
    quantity DECIMAL(20,8) NOT NULL,
    notional_value DECIMAL(20,2) NOT NULL,
    margin DECIMAL(20,2) NOT NULL,

    -- Risk
    stop_loss_price DECIMAL(20,8),
    take_profit_price DECIMAL(20,8),
    liquidation_price DECIMAL(20,8) NOT NULL,

    -- Exit
    exit_price DECIMAL(20,8),
    exit_time TIMESTAMP,
    realized_pnl DECIMAL(20,2),
    pnl_pct DECIMAL(10,4),

    -- Meta
    regime VARCHAR(20),  -- bull_trend, bear_trend, consolidation
    confidence DECIMAL(5,4),
    ai_rationale TEXT,
    status VARCHAR(20) DEFAULT 'open',  -- open, closed, liquidated

    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_positions_user_status ON positions(user_id, status);
CREATE INDEX idx_positions_symbol_status ON positions(symbol, status);
```

#### trades
```sql
CREATE TABLE trades (
    trade_id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(user_id),
    position_id INT REFERENCES positions(position_id),

    timestamp TIMESTAMP NOT NULL,
    exchange VARCHAR(20) NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    side VARCHAR(10) NOT NULL,  -- BUY or SELL (execution side)
    order_type VARCHAR(20) NOT NULL,  -- MARKET, LIMIT

    price DECIMAL(20,8) NOT NULL,
    quantity DECIMAL(20,8) NOT NULL,
    total_usdt DECIMAL(20,2) NOT NULL,
    fee DECIMAL(20,8) NOT NULL,
    fee_currency VARCHAR(10) NOT NULL,

    exchange_order_id VARCHAR(100),
    status VARCHAR(20) DEFAULT 'executed',

    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_trades_user_time ON trades(user_id, timestamp DESC);
CREATE INDEX idx_trades_position ON trades(position_id);
```

#### ai_decisions
```sql
CREATE TABLE ai_decisions (
    decision_id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(user_id),

    timestamp TIMESTAMP NOT NULL,
    agent_name VARCHAR(50) NOT NULL,

    -- Input
    input_data JSONB NOT NULL,

    -- Output
    output_data JSONB NOT NULL,

    -- Evidence (ì‹ ê·œ!)
    evidence JSONB NOT NULL,  -- ê·¼ê±° ì €ì¥
    reasoning TEXT NOT NULL,   -- AIì˜ ë…¼ë¦¬ì  ì„¤ëª…

    -- Validation (ë°±í…ŒìŠ¤íŒ…ìš©, ì‹ ê·œ!)
    actual_outcome DECIMAL(10,4),  -- ì‹¤ì œ ê²°ê³¼ (24h í›„ ì—…ë°ì´íŠ¸)
    decision_quality VARCHAR(20),  -- correct, incorrect, neutral
    evidence_accuracy JSONB,       -- ê° evidence ì •í™•ë„

    -- LLM Meta
    llm_model VARCHAR(50),
    prompt_tokens INT,
    completion_tokens INT,
    llm_cost DECIMAL(10,6),
    execution_time_ms INT,

    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_decisions_user_time ON ai_decisions(user_id, timestamp DESC);
CREATE INDEX idx_decisions_agent ON ai_decisions(agent_name, timestamp DESC);
CREATE INDEX idx_decisions_quality ON ai_decisions(decision_quality, timestamp DESC);
```

#### decision_analysis (ì‹ ê·œ)
```sql
CREATE TABLE decision_analysis (
    analysis_id SERIAL PRIMARY KEY,
    decision_id INT REFERENCES ai_decisions(decision_id),

    -- ë¶„ì„ ì‹œì  (ê±°ë˜ í›„ 24ì‹œê°„)
    analysis_timestamp TIMESTAMP NOT NULL,

    -- ê²°ê³¼
    predicted_direction VARCHAR(10),  -- LONG, SHORT, HOLD
    actual_direction VARCHAR(10),
    was_correct BOOLEAN,

    -- ì„±ê³¼
    predicted_return DECIMAL(10,4),
    actual_return DECIMAL(10,4),
    error_pct DECIMAL(10,4),

    -- ê·¼ê±° ê²€ì¦
    evidence_breakdown JSONB,  -- ê° evidenceê°€ ë§ì•˜ëŠ”ì§€

    -- ê°œì„  ì œì•ˆ
    improvement_suggestions TEXT,

    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_analysis_decision ON decision_analysis(decision_id);
CREATE INDEX idx_analysis_time ON decision_analysis(analysis_timestamp DESC);
```

#### regime_history
```sql
CREATE TABLE regime_history (
    regime_id SERIAL PRIMARY KEY,

    timestamp TIMESTAMP NOT NULL,
    regime VARCHAR(20) NOT NULL,
    confidence DECIMAL(5,4) NOT NULL,

    -- Evidence
    adx DECIMAL(10,4),
    rsi DECIMAL(10,4),
    price_vs_ma50 DECIMAL(10,4),
    trend_strength VARCHAR(20),

    ai_rationale TEXT,

    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_regime_time ON regime_history(timestamp DESC);
```

---

### 3.2 TimescaleDB Schema

#### market_data
```sql
CREATE TABLE market_data (
    time TIMESTAMPTZ NOT NULL,
    exchange VARCHAR(20) NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    timeframe VARCHAR(10) NOT NULL,  -- 1m, 5m, 15m, 1h, 4h, 1d

    open DECIMAL(20,8) NOT NULL,
    high DECIMAL(20,8) NOT NULL,
    low DECIMAL(20,8) NOT NULL,
    close DECIMAL(20,8) NOT NULL,
    volume DECIMAL(20,8) NOT NULL,

    PRIMARY KEY (time, exchange, symbol, timeframe)
);

SELECT create_hypertable('market_data', 'time');
CREATE INDEX idx_market_symbol_time ON market_data(symbol, timeframe, time DESC);
```

#### portfolio_snapshots
```sql
CREATE TABLE portfolio_snapshots (
    time TIMESTAMPTZ NOT NULL,
    user_id INT NOT NULL,

    total_value_usdt DECIMAL(20,2) NOT NULL,
    btc_value DECIMAL(20,2),
    eth_value DECIMAL(20,2),
    usdt_value DECIMAL(20,2),

    btc_allocation_pct DECIMAL(5,2),
    eth_allocation_pct DECIMAL(5,2),

    total_leverage DECIMAL(10,2),
    margin_ratio DECIMAL(10,4),
    unrealized_pnl DECIMAL(20,2),

    PRIMARY KEY (time, user_id)
);

SELECT create_hypertable('portfolio_snapshots', 'time');
```

#### funding_rate_history
```sql
CREATE TABLE funding_rate_history (
    time TIMESTAMPTZ NOT NULL,
    exchange VARCHAR(20) NOT NULL,
    symbol VARCHAR(20) NOT NULL,

    funding_rate DECIMAL(10,8) NOT NULL,
    mark_price DECIMAL(20,8) NOT NULL,
    index_price DECIMAL(20,8) NOT NULL,

    PRIMARY KEY (time, exchange, symbol)
);

SELECT create_hypertable('funding_rate_history', 'time');
```

---

### 3.3 Redis Cache Structure

#### Key Naming Convention
```
{category}:{exchange}:{symbol}:{data_type}
{category}:user:{user_id}:{data_type}
```

#### TTL Strategy
```
ì‹¤ì‹œê°„ ë°ì´í„°: TTL = ìˆ˜ì§‘ ì£¼ê¸° Ã— 1.2 (20% ì—¬ìœ )
ìš”ì•½ ë°ì´í„°: TTL = ìˆ˜ì§‘ ì£¼ê¸° Ã— 1.5 (50% ì—¬ìœ )
í¬ì§€ì…˜ ë°ì´í„°: TTL = 1ë¶„ (ìì£¼ ë³€ê²½)
```

#### Cache Keys

```yaml
# Layer 1: Market Data (High-Frequency, TTL: 6ë¶„)
market:binance:BTCUSDT:ohlcv_15m       â†’ JSON (200 candles)
market:binance:BTCUSDT:indicators      â†’ JSON (RSI, MACD, ADX, etc.)
market:binance:BTCUSDT:funding         â†’ FLOAT
market:binance:BTCUSDT:price_change_15m â†’ FLOAT

market:binance:ETHUSDT:ohlcv_15m       â†’ JSON
market:binance:ETHUSDT:indicators      â†’ JSON

# Layer 2: Contextual Data (Medium-Frequency, TTL: 45ë¶„)
news:summary                           â†’ JSON {headline, sentiment, impact, evidence}
social:summary                         â†’ JSON {score, trend, evidence, keywords}
onchain:summary                        â†’ JSON {signal, strength, evidence}

# Layer 3: User Data (TTL: 1ë¶„)
user:1:positions                       â†’ JSON Array
user:1:margin_ratio                    â†’ FLOAT
user:1:total_value                     â†’ FLOAT
user:1:has_open_position               â†’ BOOLEAN

# System State (TTL: 1ì‹œê°„)
regime:current                         â†’ JSON {regime, confidence, timestamp, evidence}
regime:history:24h                     â†’ JSON Array

# Quick Filter (TTL: 15ë¶„)
quickfilter:last_analysis_time         â†’ TIMESTAMP
quickfilter:last_trigger_reason        â†’ STRING

# Locks (TTL: 10ë¶„)
trading:lock:user:1                    â†’ STRING (execution_id)
```

---

## 4. API Specification

### 4.1 Trading APIs

#### Open Position
```http
POST /api/v2/positions/open
Content-Type: application/json

Request:
{
  "user_id": 1,
  "symbol": "BTCUSDT",
  "side": "LONG",
  "leverage": 15,
  "size_usdt": 8000,
  "stop_loss_pct": 0.03,
  "take_profit_pct": 0.10,
  "regime": "bull_trend",
  "confidence": 0.85,
  "ai_rationale": "Strong uptrend..."
}

Response (200):
{
  "success": true,
  "position_id": 1234,
  "entry_price": 68000.50,
  "quantity": 0.117647,
  "notional_value": 120000,
  "margin": 8000,
  "liquidation_price": 63235.44,
  "liquidation_distance_pct": 7.01,
  "exchange_order_id": "1234567890"
}

Response (400):
{
  "success": false,
  "error": "Liquidation distance < 15%",
  "details": {...}
}
```

#### Close Position
```http
POST /api/v2/positions/{position_id}/close
Content-Type: application/json

Request:
{
  "user_id": 1,
  "close_pct": 1.0,  // 1.0 = ì „ì²´, 0.5 = 50%
  "reason": "take_profit" | "stop_loss" | "manual" | "liquidation"
}

Response (200):
{
  "success": true,
  "position_id": 1234,
  "exit_price": 74800.00,
  "realized_pnl": 4000.00,
  "pnl_pct": 0.50,
  "holding_hours": 36
}
```

---

### 4.2 Data APIs

#### Get Cached Market Data
```http
GET /api/v2/data/market/{symbol}
Query Params:
  - timeframe: 15m (default), 1h, 4h
  - include_indicators: true (default)

Response (200):
{
  "symbol": "BTCUSDT",
  "timeframe": "15m",
  "cached_at": "2025-01-27T14:30:00Z",
  "ohlcv": [
    {
      "timestamp": "2025-01-27T14:15:00Z",
      "open": 68000,
      "high": 68200,
      "low": 67900,
      "close": 68100,
      "volume": 1250.5
    },
    ...
  ],
  "indicators": {
    "rsi": 65.3,
    "macd": {"macd": 120, "signal": 100, "histogram": 20},
    "adx": 42.1,
    "bollinger": {"upper": 69500, "middle": 68000, "lower": 66500}
  }
}
```

#### Get Current Regime
```http
GET /api/v2/regime/current

Response (200):
{
  "regime": "bull_trend",
  "confidence": 0.85,
  "timestamp": "2025-01-27T14:30:00Z",
  "duration_hours": 72,
  "evidence": {
    "adx": 42.1,
    "rsi": 65.3,
    "price_vs_ma50": "+8.5%"
  }
}
```

---

### 4.3 Portfolio APIs

#### Get Portfolio Summary
```http
GET /api/v2/users/{user_id}/portfolio

Response (200):
{
  "user_id": 1,
  "timestamp": "2025-01-27T14:30:00Z",
  "total_value_usdt": 10850.00,
  "unrealized_pnl": 850.00,
  "unrealized_pnl_pct": 8.5,

  "allocations": {
    "BTC": {
      "pct": 85.0,
      "value_usdt": 9222.50,
      "position_side": "LONG",
      "leverage": 15,
      "pnl": 750.00
    },
    "ETH": {
      "pct": 15.0,
      "value_usdt": 1627.50,
      "position_side": "HOLD",
      "leverage": 1,
      "pnl": 100.00
    }
  },

  "risk_metrics": {
    "total_leverage": 13.5,
    "margin_ratio": 0.68,
    "closest_liquidation_distance": 0.067
  }
}
```

---

## 5. Celery Task Specifications

### 5.1 Data Collection Tasks

#### Layer 1: High-Frequency (5ë¶„)

```python
@app.task(name='collect_market_data')
def collect_market_data():
    """
    ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ (OHLCV, Funding, Indicators)

    Schedule: */5 * * * * (5ë¶„ë§ˆë‹¤)
    Priority: High
    """
    exchange = ccxt.binance()

    for symbol in ['BTC/USDT', 'ETH/USDT']:
        # 1. OHLCV ìˆ˜ì§‘
        ohlcv = exchange.fetch_ohlcv(symbol, '15m', limit=200)
        redis_client.setex(
            f'market:binance:{symbol.replace("/", "")}:ohlcv_15m',
            360,  # 6ë¶„ TTL (20% ì—¬ìœ )
            json.dumps(ohlcv)
        )

        # 2. ì§€í‘œ ê³„ì‚°
        indicators = calculate_indicators(ohlcv)
        redis_client.setex(
            f'market:binance:{symbol.replace("/", "")}:indicators',
            360,
            json.dumps(indicators)
        )

        # 3. Funding Rate
        funding = exchange.fetch_funding_rate(symbol)
        redis_client.setex(
            f'market:binance:{symbol.replace("/", "")}:funding',
            360,
            funding['fundingRate']
        )

        # 4. ê°€ê²© ë³€ë™ë¥  (Quick Filterìš©)
        price_change_15m = calculate_price_change(ohlcv, minutes=15)
        redis_client.setex(
            f'market:binance:{symbol.replace("/", "")}:price_change_15m',
            360,
            price_change_15m
        )

        # 5. TimescaleDB ì €ì¥ (ë°±í…ŒìŠ¤íŒ…ìš©)
        save_to_timescaledb(symbol, ohlcv)

    return {'status': 'success', 'timestamp': time.time()}
```

#### Layer 2: Medium-Frequency (30ë¶„)

```python
@app.task(name='collect_and_summarize_news')
def collect_and_summarize_news():
    """
    ë‰´ìŠ¤ ìˆ˜ì§‘ ë° GPT-4o-mini ìš”ì•½

    Schedule: 0,30 * * * * (30ë¶„ë§ˆë‹¤)
    Priority: Medium
    """
    # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
    news_list = fetch_news_from_sources([
        'CryptoPanic',
        'CoinTelegraph',
        'CoinDesk'
    ])

    if not news_list:
        logger.warning("No news fetched")
        return {'status': 'no_news'}

    # 2. GPT-4o-mini ìš”ì•½
    summary = gpt4o_mini_summarize(news_list)

    # Expected output:
    # {
    #   "headline": "BTC ETF ìŠ¹ì¸ ì„ë°•",
    #   "sentiment": "bullish",
    #   "impact_score": 0.85,
    #   "key_facts": [...],
    #   "evidence": {
    #     "sources": [...],
    #     "reasoning": "..."
    #   }
    # }

    # 3. Redis ì €ì¥
    redis_client.setex(
        'news:summary',
        2700,  # 45ë¶„ TTL (50% ì—¬ìœ )
        json.dumps(summary)
    )

    return {'status': 'success', 'impact_score': summary['impact_score']}

@app.task(name='collect_and_analyze_social')
def collect_and_analyze_social():
    """
    ì†Œì…œ ë¯¸ë””ì–´ ìˆ˜ì§‘ ë° ê°ì„± ë¶„ì„

    Schedule: 15,45 * * * * (30ë¶„ë§ˆë‹¤, ë‰´ìŠ¤ì™€ êµëŒ€)
    Priority: Medium
    """
    # 1. ì†Œì…œ ë°ì´í„° ìˆ˜ì§‘
    tweets = fetch_crypto_tweets(keywords=['BTC', 'Bitcoin'])
    reddit_posts = fetch_reddit_posts(subreddit='cryptocurrency')

    # 2. ê°ì„± ë¶„ì„ + ìš”ì•½
    social_summary = analyze_and_summarize_social(tweets, reddit_posts)

    # Expected output:
    # {
    #   "score": 0.75,
    #   "trend": "bullish",
    #   "score_change_1h": 0.15,
    #   "evidence": {
    #     "sentiment_breakdown": {...},
    #     "key_signals": [...],
    #     "reasoning": "..."
    #   }
    # }

    # 3. Redis ì €ì¥
    redis_client.setex(
        'social:summary',
        2700,  # 45ë¶„ TTL
        json.dumps(social_summary)
    )

    return {'status': 'success', 'score': social_summary['score']}

@app.task(name='collect_and_summarize_onchain')
def collect_and_summarize_onchain():
    """
    ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘ ë° ìš”ì•½

    Schedule: 10,40 * * * * (30ë¶„ë§ˆë‹¤)
    Priority: Medium
    """
    # 1. ì˜¨ì²´ì¸ ë°ì´í„° ìˆ˜ì§‘
    exchange_flow = get_exchange_netflow()
    whale_txs = get_whale_transactions()

    # 2. ìš”ì•½ ìƒì„±
    onchain_summary = summarize_onchain_data(exchange_flow, whale_txs)

    # Expected output:
    # {
    #   "signal": "accumulation",
    #   "strength": 0.78,
    #   "evidence": {
    #     "exchange_flow": {...},
    #     "whale_activity": [...],
    #     "reasoning": "..."
    #   }
    # }

    # 3. Redis ì €ì¥
    redis_client.setex(
        'onchain:summary',
        2700,  # 45ë¶„ TTL
        json.dumps(onchain_summary)
    )

    return {'status': 'success', 'signal': onchain_summary['signal']}
```

### 5.2 Quick Filter & Trigger Tasks

#### quick_filter_and_trigger
```python
@app.task(name='quick_filter_and_trigger')
def quick_filter_and_trigger():
    """
    Quick Filter: AI ë¶„ì„ì´ í•„ìš”í•œì§€ íŒë‹¨

    Schedule: 0,15,30,45 * * * * (15ë¶„ë§ˆë‹¤)
    Priority: High
    """
    # 1. í¬ì§€ì…˜ ì²´í¬
    if has_open_position():
        logger.info("í¬ì§€ì…˜ ë³´ìœ  ì¤‘, AI ë¶„ì„ ìŠ¤í‚µ")
        return {'status': 'skipped', 'reason': 'has_position'}

    # 2. Regime ì‹ ë¢°ë„ ì²´í¬
    regime = redis_client.get('regime:current')
    if regime and regime['confidence'] < 0.8:
        logger.info("Regime ë¶ˆí™•ì‹¤, AI ë¶„ì„ íŠ¸ë¦¬ê±°")
        trigger_n8n_workflow(reason='regime_uncertain')
        return {'status': 'triggered', 'reason': 'regime_uncertain'}

    # 3. ê°€ê²© ê¸‰ë³€ ì²´í¬
    price_change = redis_client.get('market:binance:BTCUSDT:price_change_15m')
    if abs(price_change) > 1.5:
        logger.info(f"ê°€ê²© ê¸‰ë³€ {price_change:.2f}%, AI ë¶„ì„ íŠ¸ë¦¬ê±°")
        trigger_n8n_workflow(reason='price_spike')
        return {'status': 'triggered', 'reason': 'price_spike'}

    # 4. ì¤‘ìš” ë‰´ìŠ¤ ì²´í¬
    news = redis_client.get('news:summary')
    if news and news.get('impact_score', 0) > 0.8:
        logger.info("ì¤‘ìš” ë‰´ìŠ¤ ê°ì§€, AI ë¶„ì„ íŠ¸ë¦¬ê±°")
        trigger_n8n_workflow(reason='major_news')
        return {'status': 'triggered', 'reason': 'major_news'}

    # 5. ì†Œì…œ ê°ì„± ê¸‰ë³€ ì²´í¬
    social = redis_client.get('social:summary')
    if social and abs(social.get('score_change_1h', 0)) > 0.3:
        logger.info("ì†Œì…œ ê°ì„± ê¸‰ë³€, AI ë¶„ì„ íŠ¸ë¦¬ê±°")
        trigger_n8n_workflow(reason='social_shift')
        return {'status': 'triggered', 'reason': 'social_shift'}

    # 6. ì •ê¸° ì²´í¬ (4ì‹œê°„ ê²½ê³¼)
    last_analysis = redis_client.get('quickfilter:last_analysis_time')
    if time.time() - last_analysis > 4 * 3600:
        logger.info("4ì‹œê°„ ê²½ê³¼, ì •ê¸° AI ë¶„ì„ íŠ¸ë¦¬ê±°")
        trigger_n8n_workflow(reason='scheduled_check')
        return {'status': 'triggered', 'reason': 'scheduled_check'}

    # ì¡°ê±´ ë¯¸ì¶©ì¡±
    logger.info("Quick Filter ì¡°ê±´ ë¯¸ì¶©ì¡±, ìŠ¤í‚µ")
    return {'status': 'skipped', 'reason': 'no_trigger_condition'}

def trigger_n8n_workflow(reason: str):
    """n8n Webhook í˜¸ì¶œ"""
    redis_client.set('quickfilter:last_analysis_time', time.time())
    redis_client.setex('quickfilter:last_trigger_reason', 900, reason)

    response = requests.post(
        'http://n8n:5678/webhook/trading',
        json={
            'user_id': 1,
            'trigger_reason': reason,
            'timestamp': datetime.now().isoformat()
        },
        timeout=600  # 10ë¶„
    )

    return response.json()
```

#### monitor_positions
```python
@app.task(name='monitor_positions')
def monitor_positions():
    """
    í¬ì§€ì…˜ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (Stop Loss/Take Profit/ì²­ì‚° ë¦¬ìŠ¤í¬)

    Schedule: * * * * * (1ë¶„ë§ˆë‹¤)
    Priority: Critical
    """
    all_users = get_active_users()

    for user in all_users:
        positions = get_open_positions(user.user_id)

        # Redis ì—…ë°ì´íŠ¸
        redis_client.setex(
            f'user:{user.user_id}:has_open_position',
            60,
            len(positions) > 0
        )

        for pos in positions:
            current_price = get_current_price(pos.symbol)

            # 1. Stop Loss ì²´í¬
            if should_stop_loss(pos, current_price):
                close_position(pos.position_id, reason='stop_loss')
                send_alert(f"ğŸ”´ Stop Loss: {pos.symbol} @ ${current_price}")

            # 2. Take Profit ì²´í¬
            elif should_take_profit(pos, current_price):
                close_position(pos.position_id, close_pct=0.5, reason='take_profit')
                send_alert(f"ğŸŸ¢ Take Profit: {pos.symbol} @ ${current_price}")

            # 3. ì²­ì‚° ë¦¬ìŠ¤í¬ ì²´í¬
            liq_distance = calculate_liquidation_distance(pos, current_price)
            if liq_distance < 0.10:
                send_critical_alert(
                    f"âš ï¸ ì²­ì‚° ìœ„í—˜: {pos.symbol}\n"
                    f"ê±°ë¦¬: {liq_distance:.2%}\n"
                    f"ì²­ì‚°ê°€: ${pos.liquidation_price}"
                )

                if liq_distance < 0.05:
                    # ê¸´ê¸‰ 50% ì²­ì‚°
                    close_position(pos.position_id, close_pct=0.5, reason='emergency')
                    send_critical_alert(f"ğŸš¨ ê¸´ê¸‰ ì²­ì‚° ì‹¤í–‰: {pos.symbol}")
```

### 5.3 Backtesting & Analysis Tasks

#### analyze_past_decisions
```python
@app.task(name='analyze_past_decisions')
def analyze_past_decisions():
    """
    24ì‹œê°„ ì „ AI ê²°ì •ë“¤ì„ ë¶„ì„í•˜ê³  ì •í™•ë„ í‰ê°€

    Schedule: 0 0 * * * (ë§¤ì¼ 00:00 UTC)
    Priority: Medium
    """
    # 1. 24ì‹œê°„ ì „ ê²°ì • ì¡°íšŒ
    yesterday = datetime.now() - timedelta(days=1)
    decisions = db.query(AIDecision).filter(
        AIDecision.timestamp >= yesterday,
        AIDecision.timestamp < yesterday + timedelta(hours=1)
    ).all()

    results = []

    for decision in decisions:
        # 2. ì‹¤ì œ ê°€ê²© ë³€í™” ì¡°íšŒ
        actual_price_change = get_price_change_24h(
            decision.timestamp,
            decision.output_data['symbol']
        )

        # 3. ì •í™•ë„ íŒë‹¨
        predicted = decision.output_data.get('direction')
        was_correct = (
            (predicted == 'LONG' and actual_price_change > 0) or
            (predicted == 'SHORT' and actual_price_change < 0) or
            (predicted == 'HOLD' and abs(actual_price_change) < 0.01)
        )

        # 4. Evidence ê²€ì¦
        evidence_accuracy = verify_evidence(
            decision.evidence,
            actual_price_change
        )

        # 5. DB ì—…ë°ì´íŠ¸
        decision.actual_outcome = actual_price_change
        decision.decision_quality = 'correct' if was_correct else 'incorrect'
        decision.evidence_accuracy = evidence_accuracy

        # 6. DecisionAnalysis ë ˆì½”ë“œ ìƒì„±
        analysis = DecisionAnalysis(
            decision_id=decision.decision_id,
            analysis_timestamp=datetime.now(),
            predicted_direction=predicted,
            actual_direction='UP' if actual_price_change > 0 else 'DOWN',
            was_correct=was_correct,
            actual_return=actual_price_change,
            evidence_breakdown=evidence_accuracy,
            improvement_suggestions=generate_improvements(decision, was_correct)
        )
        db.add(analysis)

        results.append({
            'decision_id': decision.decision_id,
            'was_correct': was_correct,
            'evidence_accuracy': evidence_accuracy
        })

    db.commit()

    # 7. ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„± ë° Slack ë°œì†¡
    report = generate_daily_backtest_report(results)
    send_slack_report(report)

    return {'status': 'success', 'analyzed': len(results)}

def verify_evidence(evidence: dict, actual_outcome: float) -> dict:
    """ê° evidence ìš”ì†Œê°€ ì‹¤ì œë¡œ ìœ íš¨í–ˆëŠ”ì§€ ê²€ì¦"""
    accuracy = {}

    # ê¸°ìˆ ì  ì§€í‘œ
    tech = evidence.get('technical', {})
    if tech.get('adx', 0) > 40:
        # ADX > 40ì´ë©´ ê°•í•œ ì¶”ì„¸ â†’ ì‹¤ì œë¡œ í° ì›€ì§ì„ ìˆì—ˆë‚˜?
        accuracy['adx'] = abs(actual_outcome) > 0.02

    if tech.get('rsi', 50) > 70:
        # RSI ê³¼ë§¤ìˆ˜ â†’ ì‹¤ì œë¡œ í•˜ë½í–ˆë‚˜?
        accuracy['rsi'] = actual_outcome < 0

    # í€ë”ë©˜í„¸
    fund = evidence.get('fundamental', {})
    if fund.get('news_impact', 0) > 0.8:
        # ë†’ì€ ë‰´ìŠ¤ ì„íŒ©íŠ¸ â†’ ì‹¤ì œë¡œ í° ì›€ì§ì„?
        accuracy['news'] = abs(actual_outcome) > 0.03

    if fund.get('social_sentiment', 0.5) > 0.7:
        # ê¸ì • ì†Œì…œ â†’ ì‹¤ì œë¡œ ìƒìŠ¹?
        accuracy['social'] = actual_outcome > 0

    return accuracy

def generate_improvements(decision: AIDecision, was_correct: bool) -> str:
    """ê°œì„  ì œì•ˆ ìƒì„±"""
    if was_correct:
        return "Decision was correct. No improvement needed."

    suggestions = []

    # Evidence ë¶„ì„
    evidence = decision.evidence

    if evidence.get('fundamental', {}).get('social_sentiment', 0) > 0.7:
        if decision.actual_outcome < 0:
            suggestions.append("ì†Œì…œ ê°ì„± ê°€ì¤‘ì¹˜ ë‚®ì¶¤ (0.3 â†’ 0.2)")

    if evidence.get('technical', {}).get('adx', 0) < 30:
        suggestions.append("ADX < 30 êµ¬ê°„ì—ì„œëŠ” ì§„ì… ìì œ")

    return "; ".join(suggestions) if suggestions else "Further analysis needed"
```

#### Celery Beat Schedule
```python
# workers/celery_app.py

from celery.schedules import crontab

app.conf.beat_schedule = {
    # Layer 1: High-Frequency (5ë¶„)
    'collect-market-data': {
        'task': 'collect_market_data',
        'schedule': crontab(minute='*/5'),
    },

    # Layer 2: Medium-Frequency (30ë¶„)
    'summarize-news': {
        'task': 'collect_and_summarize_news',
        'schedule': crontab(minute='0,30'),
    },
    'analyze-social': {
        'task': 'collect_and_analyze_social',
        'schedule': crontab(minute='15,45'),
    },
    'summarize-onchain': {
        'task': 'collect_and_summarize_onchain',
        'schedule': crontab(minute='10,40'),
    },

    # Quick Filter & Trigger (15ë¶„)
    'quick-filter': {
        'task': 'quick_filter_and_trigger',
        'schedule': crontab(minute='0,15,30,45'),
    },

    # Position Monitoring (1ë¶„)
    'monitor-positions': {
        'task': 'monitor_positions',
        'schedule': crontab(minute='*'),
    },

    # Portfolio Update (10ë¶„)
    'update-portfolio': {
        'task': 'update_portfolio_value',
        'schedule': crontab(minute='*/10'),
    },

    # Backtesting (ë§¤ì¼ 00:00)
    'analyze-decisions': {
        'task': 'analyze_past_decisions',
        'schedule': crontab(hour=0, minute=0),
    },
}
```

---

## 6. n8n Workflow Specifications

### 6.1 Main Trading Workflow

#### Workflow Settings
```yaml
Name: Main-Trading-Workflow
Timeout: 600ì´ˆ (10ë¶„)
Error Handling: Continue on Fail (ë¡œê¹…)
Retry: ì—†ìŒ (LLMì€ ì¬ì‹œë„ ë¶ˆí•„ìš”)
```

#### Workflow Structure
```json
{
  "name": "Main-Trading-Workflow",
  "settings": {
    "executionTimeout": 600,
    "saveExecutionProgress": true
  },
  "nodes": [
    {
      "name": "Webhook Trigger",
      "type": "n8n-nodes-base.webhook",
      "parameters": {
        "path": "trading",
        "httpMethod": "POST"
      }
    },
    {
      "name": "Get Cached Data",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "url": "http://fastapi:8000/api/v2/data/market/BTCUSDT?include_summary=true",
        "timeout": 5000
      },
      "notes": "Redisì—ì„œ ìºì‹œëœ ë°ì´í„° ë¡œë“œ (ì‹œì¥/ë‰´ìŠ¤/ì†Œì…œ/ì˜¨ì²´ì¸)"
    },
    {
      "name": "CEO Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "parameters": {
        "model": "gpt-o1",
        "prompt": "Regime Detection Prompt...",
        "maxTokens": 1000
      },
      "notes": "Regime íŒë‹¨ + Evidence ìˆ˜ì§‘"
    },
    {
      "name": "Save Regime",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "method": "POST",
        "url": "http://fastapi:8000/api/v2/regime/update"
      }
    },
    {
      "name": "BTC Analyst",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "parameters": {
        "model": "gpt-4o",
        "prompt": "BTC Analysis Prompt...",
        "maxTokens": 800
      },
      "notes": "ê±°ë˜ ë°©í–¥ ê²°ì • + Evidence"
    },
    {
      "name": "Risk Chief",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "parameters": {
        "model": "gpt-4o",
        "prompt": "Risk Validation Prompt...",
        "maxTokens": 600
      },
      "notes": "ë¦¬ìŠ¤í¬ ê²€ì¦ + Approve/Veto"
    },
    {
      "name": "Save AI Decision",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "method": "POST",
        "url": "http://fastapi:8000/api/v2/decisions/save"
      },
      "notes": "evidence, reasoning í¬í•¨í•˜ì—¬ ì €ì¥"
    },
    {
      "name": "If Approved",
      "type": "n8n-nodes-base.if",
      "parameters": {
        "conditions": {
          "approved": "{{ $json.risk_chief.approval === 'APPROVED' }}"
        }
      }
    },
    {
      "name": "Execute Trade",
      "type": "n8n-nodes-base.httpRequest",
      "parameters": {
        "method": "POST",
        "url": "http://fastapi:8000/api/v2/positions/open",
        "timeout": 10000
      }
    },
    {
      "name": "Send Slack Alert",
      "type": "n8n-nodes-base.slack",
      "parameters": {
        "channel": "#trading-alerts",
        "text": "{{ $json.trade_result }}"
      }
    }
  ]
}
```

#### Timeout Handling
```javascript
// Error Handler Node (Workflow ì‹¤íŒ¨ ì‹œ)
if (error.type === 'TIMEOUT') {
  send_critical_alert({
    title: "âš ï¸ n8n Workflow Timeout",
    message: "10ë¶„ ì´ˆê³¼. AI ì‘ë‹µ ì§€ì—° ì˜ì‹¬.",
    action: "ìˆ˜ë™ í™•ì¸ í•„ìš”"
  });

  // Slackì— ì•Œë¦¼ í›„ ì¢…ë£Œ
  return { status: 'timeout', timestamp: Date.now() };
}
```

---

## 7. Infrastructure Requirements

### 7.1 Hardware Requirements (Minimum)

| Component | Spec |
|-----------|------|
| **CPU** | 4 cores |
| **RAM** | 8GB |
| **Disk** | 100GB SSD |
| **Network** | 100Mbps |

### 7.2 Docker Compose

```yaml
version: '3.8'

services:
  postgres:
    image: timescale/timescaledb:latest-pg15
    environment:
      POSTGRES_DB: axis_capital
      POSTGRES_USER: axis
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  fastapi:
    build: ./api
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://axis:${POSTGRES_PASSWORD}@postgres:5432/axis_capital
      REDIS_URL: redis://redis:6379/0
      BINANCE_API_KEY: ${BINANCE_API_KEY}
      BINANCE_API_SECRET: ${BINANCE_API_SECRET}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
    depends_on:
      - postgres
      - redis

  celery-worker:
    build: ./api
    command: celery -A workers.celery_app worker -l info
    environment:
      DATABASE_URL: postgresql://axis:${POSTGRES_PASSWORD}@postgres:5432/axis_capital
      REDIS_URL: redis://redis:6379/0
    depends_on:
      - redis
      - postgres

  celery-beat:
    build: ./api
    command: celery -A workers.celery_app beat -l info
    environment:
      REDIS_URL: redis://redis:6379/0
    depends_on:
      - redis

  n8n:
    image: n8nio/n8n:latest
    ports:
      - "5678:5678"
    environment:
      N8N_BASIC_AUTH_USER: admin
      N8N_BASIC_AUTH_PASSWORD: ${N8N_PASSWORD}
    volumes:
      - n8n_data:/home/node/.n8n
    depends_on:
      - fastapi

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana

volumes:
  postgres_data:
  redis_data:
  n8n_data:
  grafana_data:
```

---

## 8. Security Requirements

### 8.1 API Key Management

```python
# api/core/security.py
from cryptography.fernet import Fernet

class APIKeyManager:
    def __init__(self):
        self.cipher = Fernet(os.getenv('ENCRYPTION_KEY'))

    def encrypt(self, api_key: str) -> str:
        return self.cipher.encrypt(api_key.encode()).decode()

    def decrypt(self, encrypted: str) -> str:
        return self.cipher.decrypt(encrypted.encode()).decode()

# Usage
api_key = "user_binance_api_key"
encrypted = api_key_manager.encrypt(api_key)
# DBì— ì €ì¥: encrypted

# ì‚¬ìš© ì‹œ
decrypted = api_key_manager.decrypt(encrypted)
binance = ccxt.binance({'apiKey': decrypted, ...})
```

### 8.2 Rate Limiting

```python
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter

@app.post("/api/v2/positions/open")
@limiter(times=10, seconds=60)  # 10 requests per minute
async def open_position(...):
    pass
```

### 8.3 Authentication

```python
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

def verify_token(credentials: HTTPAuthorizationCredentials):
    token = credentials.credentials
    # JWT ê²€ì¦
    payload = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
    return payload['user_id']

@app.get("/api/v2/users/{user_id}/portfolio")
async def get_portfolio(
    user_id: int,
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    verified_user_id = verify_token(credentials)
    if verified_user_id != user_id:
        raise HTTPException(403, "Forbidden")
    ...
```

---

## 9. Monitoring & Observability

### 9.1 Prometheus Metrics

```python
from prometheus_client import Counter, Histogram, Gauge

# Metrics
trades_total = Counter('trades_total', 'Total trades executed', ['side', 'symbol'])
trade_pnl = Histogram('trade_pnl', 'Trade P&L distribution')
open_positions = Gauge('open_positions', 'Number of open positions', ['user_id'])
liquidation_distance = Gauge('liquidation_distance', 'Closest liquidation distance', ['user_id'])

# Usage
trades_total.labels(side='LONG', symbol='BTCUSDT').inc()
trade_pnl.observe(pnl_value)
```

### 9.2 Logging

```python
import structlog

logger = structlog.get_logger()

logger.info(
    "position_opened",
    user_id=1,
    symbol="BTCUSDT",
    side="LONG",
    leverage=15,
    entry_price=68000
)

logger.error(
    "position_liquidated",
    user_id=1,
    position_id=1234,
    symbol="BTCUSDT",
    loss=-8000
)
```

---

## 10. Cost Analysis & Optimization

### 10.1 LLM Cost Breakdown

#### GPT-4o-mini (Summarization)
```
Purpose: ë‰´ìŠ¤/ì†Œì…œ/ì˜¨ì²´ì¸ ìš”ì•½
Frequency: 48íšŒ/ì¼ (30ë¶„ë§ˆë‹¤)
Input: 500 tokens avg
Output: 100 tokens avg
Cost: $0.15/1M input, $0.60/1M output

Daily: 48 Ã— (500Ã—0.15 + 100Ã—0.60) / 1,000,000
     = $0.048/ì¼
Monthly: $1.44
```

#### GPT-o1 (CEO Agent)
```
Purpose: Regime Detection
Frequency: 15íšŒ/ì¼ (Quick Filter í†µê³¼ ì‹œ)
Input: 800 tokens avg
Output: 200 tokens avg
Cost: $15/1M input, $60/1M output

Daily: 15 Ã— (800Ã—15 + 200Ã—60) / 1,000,000
     = $0.36/ì¼
Monthly: $10.80
```

#### GPT-4o (Analysts & Risk)
```
Purpose: BTC Analyst + Risk Chief
Frequency: 30íšŒ/ì¼ (ê° 15íšŒ)
Input: 600 tokens avg
Output: 150 tokens avg
Cost: $2.5/1M input, $10/1M output

Daily: 30 Ã— (600Ã—2.5 + 150Ã—10) / 1,000,000
     = $0.09/ì¼
Monthly: $2.70
```

### 10.2 Infrastructure Cost

| í•­ëª© | ìŠ¤í™ | ì›” ë¹„ìš© |
|-----|-----|--------|
| VPS (DigitalOcean) | 8GB RAM, 4 vCPU | $30 |
| Redis Cloud | 256MB | $7 |
| PostgreSQL (VPS í¬í•¨) | - | $0 |
| Bandwidth | ~100GB | $0 (í¬í•¨) |
| **í•©ê³„** | | **$37** |

### 10.3 Total Monthly Cost

| Category | í•­ëª© | ì›” ë¹„ìš© |
|----------|-----|--------|
| **LLM** | GPT-4o-mini | $1.44 |
| | GPT-o1 | $10.80 |
| | GPT-4o | $2.70 |
| **ì¸í”„ë¼** | VPS | $30.00 |
| | Redis | $7.00 |
| **ì´ ìš´ì˜ë¹„** | | **$51.94** |

### 10.4 Trading Cost (ë³„ë„)

```
ìë³¸: $10,000
ê±°ë˜: ì›” 20íšŒ (Quick Filter í†µê³¼ + AI ìŠ¹ì¸)
í¬ì§€ì…˜ í¬ê¸°: $8,000 avg
ë ˆë²„ë¦¬ì§€: í‰ê·  12x

Binance ìˆ˜ìˆ˜ë£Œ: 0.04% (Maker)
  $8,000 Ã— 20íšŒ Ã— 0.0004 = $64

í€ë”© ë¹„ìš©: 8ì‹œê°„ë§ˆë‹¤ Â±0.01%
  í‰ê·  ë³´ìœ  24ì‹œê°„ Ã— 3íšŒ Ã— 0.01% Ã— $96,000 = ~$30

ìŠ¬ë¦¬í”¼ì§€: 0.05% (ì‹œì¥ê°€)
  $8,000 Ã— 20íšŒ Ã— 0.0005 = $8

ì´ ê±°ë˜ ë¹„ìš©: ~$102/ì›”
```

### 10.5 ROI Analysis

```yaml
ì´ˆê¸° ìë³¸: $10,000
ëª©í‘œ ìˆ˜ìµë¥ : ì›” 10% ($1,000)

ë¹„ìš©:
  - ìš´ì˜ë¹„: $52
  - ê±°ë˜ë¹„: $102
  - í•©ê³„: $154

ìˆœìˆ˜ìµ: $1,000 - $154 = $846

ì‹¤ì œ ROI: 8.46%/ì›” (ì—° 170%)
```

### 10.6 Cost Optimization

#### Quick Filter íš¨ê³¼
```
Without Quick Filter:
  - n8n ì‹¤í–‰: 96íšŒ/ì¼
  - LLM ë¹„ìš©: ~$600/ì›”

With Quick Filter:
  - n8n ì‹¤í–‰: 15íšŒ/ì¼
  - LLM ë¹„ìš©: ~$15/ì›”

ì ˆê°: $585/ì›” (97%)
```

#### Caching íš¨ê³¼
```
Without Cache:
  - n8nì´ ì§ì ‘ API í˜¸ì¶œ
  - ì§€ì—°: ~5-10ì´ˆ
  - API Rate Limit ìœ„í—˜

With Cache:
  - Redisì—ì„œ ì¦‰ì‹œ ë¡œë“œ
  - ì§€ì—°: ~100ms
  - API í˜¸ì¶œ 0íšŒ (n8nì—ì„œ)

ì ˆê°: ì‹œê°„ 98%, ì•ˆì •ì„± í–¥ìƒ
```

---

## 11. Performance Requirements

| Metric | Target | Critical |
|--------|--------|----------|
| **API P95 Latency** | < 500ms | < 2s |
| **Order Execution** | < 10s | < 30s |
| **Data Freshness** | < 5s | < 30s |
| **LLM Response** | < 20s | < 60s |
| **Cache Hit Rate** | > 90% | > 80% |
| **DB Query Time** | < 100ms | < 500ms |
| **n8n Workflow** | < 2ë¶„ | < 10ë¶„ (timeout) |

---

## 12. Failure Handling

### 12.1 Data Collection Failures

#### News API Down
```python
@app.task(name='collect_and_summarize_news')
def collect_and_summarize_news():
    try:
        news_list = fetch_news_from_sources()
    except Exception as e:
        logger.error(f"News fetch failed: {e}")

        # Fallback: ì´ì „ ìš”ì•½ ì‚¬ìš©
        previous = redis_client.get('news:summary')
        if previous:
            logger.warning("Using previous news summary")
            return {'status': 'fallback', 'source': 'previous'}

        # ì´ì „ ê²ƒë„ ì—†ìœ¼ë©´: Neutral ìƒíƒœ
        redis_client.setex(
            'news:summary',
            2700,
            json.dumps({
                'headline': 'Data unavailable',
                'sentiment': 'neutral',
                'impact_score': 0.5,
                'evidence': {'reasoning': 'News API failed'}
            })
        )

        send_alert("âš ï¸ News API ì‹¤íŒ¨, Fallback ì‚¬ìš©")
        return {'status': 'failed_fallback'}
```

#### Summarization LLM Failed
```python
def gpt4o_mini_summarize(news_list):
    try:
        summary = openai_client.summarize(news_list)
        return summary
    except Exception as e:
        logger.error(f"GPT-4o-mini failed: {e}")

        # Fallback: ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ìš”ì•½
        summary = rule_based_summarize(news_list)
        summary['evidence']['reasoning'] += " (Fallback: Rule-based)"

        send_alert("âš ï¸ LLM ìš”ì•½ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ ì‚¬ìš©")
        return summary
```

### 12.2 n8n Workflow Failures

#### Timeout
```python
# Celery trigger
try:
    response = requests.post(
        'http://n8n:5678/webhook/trading',
        json={...},
        timeout=600  # 10ë¶„
    )
except requests.Timeout:
    logger.critical("n8n workflow timeout")
    send_critical_alert("ğŸš¨ n8n Timeout (10ë¶„ ì´ˆê³¼)")

    # Redisì— ì‹¤íŒ¨ ê¸°ë¡
    redis_client.setex(
        'quickfilter:last_failure',
        3600,
        json.dumps({'reason': 'timeout', 'time': time.time()})
    )

    return {'status': 'timeout'}
```

#### LLM Error in Workflow
```javascript
// n8n Error Handler
if (error.type === 'AI_ERROR') {
  // CEOê°€ ì‹¤íŒ¨í•˜ë©´ ì „ì²´ ì¤‘ë‹¨
  if (node === 'CEO Agent') {
    send_alert("âš ï¸ CEO Agent ì‹¤íŒ¨, ê±°ë˜ ìŠ¤í‚µ");
    return { status: 'aborted', reason: 'ceo_failed' };
  }

  // Analyst ì‹¤íŒ¨ â†’ HOLD
  if (node === 'BTC Analyst') {
    return {
      decision: 'HOLD',
      reason: 'analyst_failed',
      evidence: { error: error.message }
    };
  }
}
```

### 12.3 Redis Failure

```python
def get_cached_data_safe(key: str):
    """Redis ì‹¤íŒ¨ ì‹œ DB Fallback"""
    try:
        data = redis_client.get(key)
        if data:
            return data
    except Exception as e:
        logger.error(f"Redis failed: {e}")
        send_critical_alert("ğŸš¨ Redis ì—°ê²° ì‹¤íŒ¨")

    # Fallback to PostgreSQL
    logger.warning("Falling back to PostgreSQL")
    return get_from_postgresql(key)
```

---

## 13. Disaster Recovery

### 11.1 Backup Strategy

```yaml
PostgreSQL:
  - Full Backup: ë§¤ì¼ 03:00 (S3)
  - Incremental: ë§¤ì‹œê°„ (ë¡œì»¬)
  - Retention: 30ì¼

Redis:
  - RDB Snapshot: ë§¤ì‹œê°„
  - AOF: ì‹¤ì‹œê°„ (appendfsync everysec)
  - Retention: 7ì¼

Code:
  - Git: ëª¨ë“  ë³€ê²½ì‚¬í•­
  - Docker Images: Tagged versions
```

### 11.2 Failover Plan

```yaml
Binance API Down:
  1. ìë™ ê°ì§€ (5ë¶„ ë‚´)
  2. Bybit APIë¡œ ì „í™˜
  3. í¬ì§€ì…˜ ë¯¸ëŸ¬ë§
  4. ì•Œë¦¼ ë°œì†¡

Database Failure:
  1. Read Replicaë¡œ ì „í™˜ (ì½ê¸° ì „ìš©)
  2. ë°±ì—…ì—ì„œ ë³µêµ¬ (ìµœëŒ€ 1ì‹œê°„ ë°ì´í„° ì†ì‹¤)

Redis Failure:
  1. PostgreSQLë¡œ Fallback (ëŠë¦¬ì§€ë§Œ ì‘ë™)
  2. Redis ì¬ì‹œì‘
  3. ìºì‹œ Warm-up
```

---

## 12. Testing Strategy

### 12.1 Unit Tests

```python
# tests/test_risk.py
def test_liquidation_distance_calculation():
    position = Position(
        side='LONG',
        entry_price=68000,
        leverage=15,
        mark_price=70000
    )

    distance = calculate_liquidation_distance(position)

    assert distance > 0.10  # > 10%
    assert distance < 0.15  # < 15%
```

### 12.2 Integration Tests

```python
# tests/test_trading_flow.py
async def test_full_trading_cycle():
    # 1. Trigger workflow
    response = await client.post("/api/v2/trading/trigger", json={'user_id': 1})
    assert response.status_code == 200

    # 2. Wait for execution
    await asyncio.sleep(60)

    # 3. Check position opened
    positions = await get_open_positions(user_id=1)
    assert len(positions) > 0

    # 4. Verify risk limits
    for pos in positions:
        assert pos.liquidation_distance > 0.15
```

### 12.3 Load Tests

```bash
# locust ì‚¬ìš©
locust -f tests/load_test.py --users 100 --spawn-rate 10
```

---

## 13. Deployment

### 13.1 CI/CD Pipeline

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run Tests
        run: |
          pip install -r requirements.txt
          pytest tests/

  deploy:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to Server
        run: |
          ssh user@server 'cd /app && git pull && docker-compose up -d --build'
```

---

## 14. Implementation Workflow

### 14.1 Step-by-Step Approval Process

#### Workflow Rules

```python
class ImplementationWorkflow:
    """êµ¬í˜„ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬"""

    def __init__(self):
        self.current_task = None
        self.approval_required = True

    def start_task(self, task_name: str):
        """ì‘ì—… ì‹œì‘"""
        if self.current_task:
            raise Exception("ì´ì „ ì‘ì—… ì™„ë£Œ í›„ ì‹œì‘ ê°€ëŠ¥")

        self.current_task = task_name
        print(f"ğŸš€ ì‘ì—… ì‹œì‘: {task_name}")

    def complete_task(self, details: dict):
        """ì‘ì—… ì™„ë£Œ ë³´ê³ """
        report = f"""
        âœ… ì™„ë£Œ: {self.current_task}
        ğŸ“ ë‚´ìš©: {details['description']}
        ğŸ§ª í…ŒìŠ¤íŠ¸: {details['test_result']}
        ğŸ“‚ íŒŒì¼: {details['files_changed']}
        â“ í™•ì¸: ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰í•´ë„ ë ê¹Œìš”?
        """
        print(report)

        # Owner ìŠ¹ì¸ ëŒ€ê¸°
        self.wait_for_approval()

    def wait_for_approval(self):
        """ìŠ¹ì¸ ëŒ€ê¸°"""
        print("â³ Owner ìŠ¹ì¸ ëŒ€ê¸° ì¤‘...")
        # Ownerê°€ ìŠ¹ì¸í•˜ë©´ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì²´í¬

    def approve(self):
        """ìŠ¹ì¸ (Ownerë§Œ ê°€ëŠ¥)"""
        print(f"âœ“ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì²´í¬: {self.current_task}")
        self.current_task = None

    def reject(self, reason: str):
        """ê±°ë¶€"""
        print(f"âŒ ì¬ì‘ì—… í•„ìš”: {reason}")
        # í˜„ì¬ ì‘ì—… ì¬ì‹œë„
```

#### Task Granularity (ì‘ì—… ë‹¨ìœ„)

```yaml
Good (ì ì ˆí•œ í¬ê¸°):
  - "Docker Compose íŒŒì¼ ì‘ì„±"
  - "PostgreSQL í…Œì´ë¸” 1ê°œ ìƒì„±"
  - "Celery Task 1ê°œ êµ¬í˜„"
  - "n8n ë…¸ë“œ 3ê°œ ì¶”ê°€"

Bad (ë„ˆë¬´ í¼):
  - "ì „ì²´ ì¸í”„ë¼ êµ¬ì¶•" â†’ 10ê°œ ì‘ì—…ìœ¼ë¡œ ìª¼ê°œê¸°
  - "ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ì„±" â†’ ê° Taskë³„ë¡œ ìª¼ê°œê¸°
  - "n8n Workflow ì „ì²´" â†’ ë…¸ë“œë³„ë¡œ ìª¼ê°œê¸°

Bad (ë„ˆë¬´ ì‘ìŒ):
  - "ë³€ìˆ˜ 1ê°œ ì¶”ê°€" â†’ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¬¶ê¸°
  - "ì£¼ì„ ì‘ì„±" â†’ ì½”ë“œ ì‘ì„±ê³¼ í•¨ê»˜
```

#### Example: Phase 1 Execution

```bash
# Task 1
AI: "ì‘ì—… ì‹œì‘: Docker Compose ì„¤ì •"
AI: [docker-compose.yml ì‘ì„±]
AI: [í…ŒìŠ¤íŠ¸ ì‹¤í–‰: docker-compose up -d]
AI:
  âœ… ì™„ë£Œ: Docker Compose ì„¤ì •
  ğŸ“ ë‚´ìš©: PostgreSQL, Redis, n8n ì»¨í…Œì´ë„ˆ ì„¤ì • ì™„ë£Œ
  ğŸ§ª í…ŒìŠ¤íŠ¸: ëª¨ë“  ì»¨í…Œì´ë„ˆ RUNNING ìƒíƒœ í™•ì¸
  ğŸ“‚ íŒŒì¼: docker-compose.yml
  â“ í™•ì¸: ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰í•´ë„ ë ê¹Œìš”?

Owner: "í™•ì¸í–ˆì–´, ì§„í–‰í•´"

AI: [ì²´í¬ë¦¬ìŠ¤íŠ¸ ì²´í¬ âœ“]

# Task 2
AI: "ì‘ì—… ì‹œì‘: PostgreSQL ì—°ê²° í…ŒìŠ¤íŠ¸"
...
```

#### Error Handling

```yaml
ë¬¸ì œ ë°œìƒ ì‹œ:
  1. ì¦‰ì‹œ Ownerì—ê²Œ ë³´ê³ 
     "âš ï¸ ì—ëŸ¬ ë°œìƒ: [ì—ëŸ¬ ë‚´ìš©]"

  2. ì—ëŸ¬ ìƒì„¸ ì •ë³´
     - ì—ëŸ¬ ë©”ì‹œì§€
     - ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤
     - ì¬í˜„ ë°©ë²•

  3. í•´ê²° ë°©ì•ˆ ì œì•ˆ
     - Option A: [ë°©ì•ˆ 1]
     - Option B: [ë°©ì•ˆ 2]

  4. Owner ì§€ì‹œ ëŒ€ê¸°
     - ì¬ì‹œë„
     - ë‹¤ë¥¸ ë°©ë²•
     - ìŠ¤í‚µ (ë‚˜ì¤‘ì—)

ì˜ˆì‹œ:
  âš ï¸ ì—ëŸ¬: PostgreSQL ì—°ê²° ì‹¤íŒ¨
  ì›ì¸: Password ë¶ˆì¼ì¹˜
  í•´ê²°ì•ˆ:
    A) .env íŒŒì¼ ìˆ˜ì •
    B) docker-compose ì¬ì‹œì‘
  ì–´ë–»ê²Œ í• ê¹Œìš”?
```

---

## 15. Appendix

### 15.1 Environment Variables

```bash
# .env.example
DATABASE_URL=postgresql://axis:password@localhost:5432/axis_capital
REDIS_URL=redis://localhost:6379/0

BINANCE_API_KEY=your_binance_api_key
BINANCE_API_SECRET=your_binance_api_secret

OPENAI_API_KEY=your_openai_api_key

ENCRYPTION_KEY=your_encryption_key_for_api_keys

JWT_SECRET_KEY=your_jwt_secret

SLACK_WEBHOOK_URL=your_slack_webhook
```

---

**Document Status**: Ready for Implementation
**Next Step**: Implementation Checklist ì‘ì„±

